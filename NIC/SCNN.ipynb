{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nir in f:\\anaconda\\lib\\site-packages (1.0.4)\n",
      "Requirement already satisfied: numpy in f:\\anaconda\\lib\\site-packages (from nir) (1.26.4)\n",
      "Requirement already satisfied: h5py in f:\\anaconda\\lib\\site-packages (from nir) (3.11.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in f:\\anaconda\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in f:\\anaconda\\lib\\site-packages (0.21.0)\n",
      "Requirement already satisfied: filelock in f:\\anaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in f:\\anaconda\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in f:\\anaconda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in f:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in f:\\anaconda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in f:\\anaconda\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in f:\\anaconda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in f:\\anaconda\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in f:\\anaconda\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snntorch in f:\\anaconda\\lib\\site-packages (0.9.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install snntorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in f:\\anaconda\\lib\\site-packages (2.6.0)\n",
      "Requirement already satisfied: torchvision in f:\\anaconda\\lib\\site-packages (0.21.0)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp312-cp312-win_amd64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: filelock in f:\\anaconda\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in f:\\anaconda\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in f:\\anaconda\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in f:\\anaconda\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in f:\\anaconda\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in f:\\anaconda\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in f:\\anaconda\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in f:\\anaconda\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in f:\\anaconda\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in f:\\anaconda\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Collecting torch\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in f:\\anaconda\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchaudio-2.6.0%2Bcu126-cp312-cp312-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 0.0/4.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.2 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 1.6/4.2 MB 5.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 3.4/4.2 MB 6.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.2/4.2 MB 7.6 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cu126/torch-2.6.0%2Bcu126-cp312-cp312-win_amd64.whl (2496.1 MB)\n",
      "   ---------------------------------------- 0.0/2.5 GB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.5 GB 13.4 MB/s eta 0:03:07\n",
      "   ---------------------------------------- 0.0/2.5 GB 13.3 MB/s eta 0:03:08\n",
      "   ---------------------------------------- 0.0/2.5 GB 14.4 MB/s eta 0:02:53\n",
      "   ---------------------------------------- 0.0/2.5 GB 15.1 MB/s eta 0:02:45\n",
      "   ---------------------------------------- 0.0/2.5 GB 16.2 MB/s eta 0:02:33\n",
      "   ---------------------------------------- 0.0/2.5 GB 17.2 MB/s eta 0:02:24\n",
      "   ---------------------------------------- 0.0/2.5 GB 18.3 MB/s eta 0:02:16\n",
      "    --------------------------------------- 0.0/2.5 GB 20.8 MB/s eta 0:01:59\n",
      "    --------------------------------------- 0.0/2.5 GB 24.1 MB/s eta 0:01:42\n",
      "    --------------------------------------- 0.1/2.5 GB 27.3 MB/s eta 0:01:30\n",
      "   - -------------------------------------- 0.1/2.5 GB 30.2 MB/s eta 0:01:21\n",
      "   - -------------------------------------- 0.1/2.5 GB 34.5 MB/s eta 0:01:10\n",
      "   - -------------------------------------- 0.1/2.5 GB 40.8 MB/s eta 0:00:59\n",
      "   -- ------------------------------------- 0.1/2.5 GB 46.4 MB/s eta 0:00:52\n",
      "   -- ------------------------------------- 0.2/2.5 GB 51.2 MB/s eta 0:00:46\n",
      "   -- ------------------------------------- 0.2/2.5 GB 55.4 MB/s eta 0:00:42\n",
      "   --- ------------------------------------ 0.2/2.5 GB 59.2 MB/s eta 0:00:39\n",
      "   --- ------------------------------------ 0.2/2.5 GB 62.4 MB/s eta 0:00:37\n",
      "   --- ------------------------------------ 0.2/2.5 GB 65.4 MB/s eta 0:00:35\n",
      "   ---- ----------------------------------- 0.3/2.5 GB 80.6 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 0.3/2.5 GB 101.6 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 0.3/2.5 GB 111.7 MB/s eta 0:00:20\n",
      "   ----- ---------------------------------- 0.3/2.5 GB 117.2 MB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 0.4/2.5 GB 115.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 0.4/2.5 GB 115.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 0.4/2.5 GB 115.6 MB/s eta 0:00:19\n",
      "   ------ --------------------------------- 0.4/2.5 GB 115.6 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 0.5/2.5 GB 114.8 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 0.5/2.5 GB 114.8 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 0.5/2.5 GB 114.8 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 0.5/2.5 GB 114.8 MB/s eta 0:00:18\n",
      "   -------- ------------------------------- 0.6/2.5 GB 114.8 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 0.6/2.5 GB 114.8 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 0.6/2.5 GB 111.0 MB/s eta 0:00:18\n",
      "   --------- ------------------------------ 0.6/2.5 GB 113.3 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 0.6/2.5 GB 114.0 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 0.7/2.5 GB 114.0 MB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 0.7/2.5 GB 113.3 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 0.7/2.5 GB 114.0 MB/s eta 0:00:16\n",
      "   ----------- ---------------------------- 0.7/2.5 GB 114.0 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 0.8/2.5 GB 114.0 MB/s eta 0:00:16\n",
      "   ------------ --------------------------- 0.8/2.5 GB 114.0 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 0.8/2.5 GB 114.1 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 0.8/2.5 GB 114.0 MB/s eta 0:00:15\n",
      "   ------------- -------------------------- 0.9/2.5 GB 118.0 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 0.9/2.5 GB 118.9 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 0.9/2.5 GB 118.0 MB/s eta 0:00:14\n",
      "   -------------- ------------------------- 0.9/2.5 GB 118.0 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 1.0/2.5 GB 118.0 MB/s eta 0:00:14\n",
      "   --------------- ------------------------ 1.0/2.5 GB 118.1 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 1.0/2.5 GB 117.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 1.0/2.5 GB 117.2 MB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 1.0/2.5 GB 117.2 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 1.1/2.5 GB 118.0 MB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 1.1/2.5 GB 118.0 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 1.1/2.5 GB 118.0 MB/s eta 0:00:12\n",
      "   ----------------- ---------------------- 1.1/2.5 GB 104.8 MB/s eta 0:00:14\n",
      "   ------------------ --------------------- 1.1/2.5 GB 104.8 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 1.2/2.5 GB 104.8 MB/s eta 0:00:13\n",
      "   ------------------ --------------------- 1.2/2.5 GB 104.8 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 1.2/2.5 GB 104.8 MB/s eta 0:00:13\n",
      "   ------------------- -------------------- 1.2/2.5 GB 104.8 MB/s eta 0:00:13\n",
      "   -------------------- ------------------- 1.3/2.5 GB 104.8 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 1.3/2.5 GB 104.8 MB/s eta 0:00:12\n",
      "   -------------------- ------------------- 1.3/2.5 GB 104.8 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 1.3/2.5 GB 104.8 MB/s eta 0:00:12\n",
      "   --------------------- ------------------ 1.4/2.5 GB 104.8 MB/s eta 0:00:11\n",
      "   ---------------------- ----------------- 1.4/2.5 GB 118.9 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 1.4/2.5 GB 118.9 MB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 1.4/2.5 GB 118.8 MB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 1.4/2.5 GB 118.9 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 1.5/2.5 GB 118.0 MB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 1.5/2.5 GB 118.0 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 1.5/2.5 GB 118.9 MB/s eta 0:00:09\n",
      "   ------------------------ --------------- 1.5/2.5 GB 118.0 MB/s eta 0:00:09\n",
      "   ------------------------- -------------- 1.6/2.5 GB 118.0 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 1.6/2.5 GB 118.0 MB/s eta 0:00:08\n",
      "   ------------------------- -------------- 1.6/2.5 GB 118.9 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 1.6/2.5 GB 118.0 MB/s eta 0:00:08\n",
      "   -------------------------- ------------- 1.7/2.5 GB 118.9 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.7/2.5 GB 118.9 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.7/2.5 GB 118.9 MB/s eta 0:00:07\n",
      "   --------------------------- ------------ 1.7/2.5 GB 118.9 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.8/2.5 GB 118.8 MB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 1.8/2.5 GB 118.0 MB/s eta 0:00:07\n",
      "   ----------------------------- ---------- 1.8/2.5 GB 118.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.8/2.5 GB 118.0 MB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 1.9/2.5 GB 118.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.9/2.5 GB 118.0 MB/s eta 0:00:06\n",
      "   ------------------------------ --------- 1.9/2.5 GB 118.1 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 1.9/2.5 GB 118.9 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 2.0/2.5 GB 118.8 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 2.0/2.5 GB 114.8 MB/s eta 0:00:05\n",
      "   ------------------------------- -------- 2.0/2.5 GB 112.5 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 2.0/2.5 GB 112.5 MB/s eta 0:00:05\n",
      "   -------------------------------- ------- 2.0/2.5 GB 112.5 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 2.1/2.5 GB 112.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 2.1/2.5 GB 112.5 MB/s eta 0:00:04\n",
      "   --------------------------------- ------ 2.1/2.5 GB 112.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.1/2.5 GB 112.4 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.2/2.5 GB 109.5 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 2.2/2.5 GB 109.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 2.2/2.5 GB 108.8 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 2.2/2.5 GB 109.5 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 2.2/2.5 GB 111.7 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.3/2.5 GB 111.0 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 2.3/2.5 GB 111.0 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 2.3/2.5 GB 111.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 2.3/2.5 GB 111.0 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 2.4/2.5 GB 111.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.4/2.5 GB 111.0 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 2.4/2.5 GB 110.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.4/2.5 GB 113.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.5 GB 112.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 111.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 110.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.5/2.5 GB 116.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.5/2.5 GB 32.6 MB/s eta 0:00:00\n",
      "Installing collected packages: torch, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0\n",
      "    Uninstalling torch-2.6.0:\n",
      "      Successfully uninstalled torch-2.6.0\n",
      "Successfully installed torch-2.6.0+cu126 torchaudio-2.6.0+cu126\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'F:\\anaconda\\Lib\\site-packages\\~orch'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def check_cuda():\n",
    "    print(torch.version.cuda)\n",
    "    cuda_is_ok = torch.cuda.is_available()\n",
    "    print(f\"CUDA Enabled: {cuda_is_ok}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.6\n",
      "CUDA Enabled: True\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(check_cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened size for fc1: 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 8/469 [00:04<03:51,  1.99it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 256\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m    255\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m model \u001b[38;5;241m=\u001b[39m main()\n",
      "Cell \u001b[1;32mIn[14], line 246\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    243\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m load_nmnist()\n\u001b[0;32m    245\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m train(model, train_loader, optimizer)\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# Test the model\u001b[39;00m\n\u001b[0;32m    249\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m test(model, test_loader)\n",
      "Cell \u001b[1;32mIn[14], line 166\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, optimizer, num_steps)\u001b[0m\n\u001b[0;32m    163\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    164\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 166\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, targets \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m    167\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    168\u001b[0m     targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\tqdm\\std.py:1191\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1189\u001b[0m dt \u001b[38;5;241m=\u001b[39m cur_t \u001b[38;5;241m-\u001b[39m last_print_t\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m mininterval \u001b[38;5;129;01mand\u001b[39;00m cur_t \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m min_start_t:\n\u001b[1;32m-> 1191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate(n \u001b[38;5;241m-\u001b[39m last_print_n)\n\u001b[0;32m   1192\u001b[0m     last_print_n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_n\n\u001b[0;32m   1193\u001b[0m     last_print_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlast_print_t\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\tqdm\\std.py:1242\u001b[0m, in \u001b[0;36mtqdm.update\u001b[1;34m(self, n)\u001b[0m\n\u001b[0;32m   1240\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dn(dn)\n\u001b[0;32m   1241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ema_dt(dt)\n\u001b[1;32m-> 1242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrefresh(lock_args\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlock_args)\n\u001b[0;32m   1243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdynamic_miniters:\n\u001b[0;32m   1244\u001b[0m     \u001b[38;5;66;03m# If no `miniters` was specified, adjust automatically to the\u001b[39;00m\n\u001b[0;32m   1245\u001b[0m     \u001b[38;5;66;03m# maximum iteration rate seen so far between two prints.\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m     \u001b[38;5;66;03m# e.g.: After running `tqdm.update(5)`, subsequent\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m     \u001b[38;5;66;03m# calls to `tqdm.update()` will only cause an update after\u001b[39;00m\n\u001b[0;32m   1248\u001b[0m     \u001b[38;5;66;03m# at least 5 more iterations.\u001b[39;00m\n\u001b[0;32m   1249\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval \u001b[38;5;129;01mand\u001b[39;00m dt \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxinterval:\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\tqdm\\std.py:1347\u001b[0m, in \u001b[0;36mtqdm.refresh\u001b[1;34m(self, nolock, lock_args)\u001b[0m\n\u001b[0;32m   1345\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1346\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39macquire()\n\u001b[1;32m-> 1347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay()\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nolock:\n\u001b[0;32m   1349\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\tqdm\\std.py:1495\u001b[0m, in \u001b[0;36mtqdm.display\u001b[1;34m(self, msg, pos)\u001b[0m\n\u001b[0;32m   1493\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1494\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(pos)\n\u001b[1;32m-> 1495\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__str__\u001b[39m() \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m msg)\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pos:\n\u001b[0;32m   1497\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmoveto(\u001b[38;5;241m-\u001b[39mpos)\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\tqdm\\std.py:459\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.print_status\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprint_status\u001b[39m(s):\n\u001b[0;32m    458\u001b[0m     len_s \u001b[38;5;241m=\u001b[39m disp_len(s)\n\u001b[1;32m--> 459\u001b[0m     fp_write(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m s \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mmax\u001b[39m(last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m len_s, \u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m    460\u001b[0m     last_len[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m len_s\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\tqdm\\std.py:452\u001b[0m, in \u001b[0;36mtqdm.status_printer.<locals>.fp_write\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfp_write\u001b[39m(s):\n\u001b[1;32m--> 452\u001b[0m     fp\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(s))\n\u001b[0;32m    453\u001b[0m     fp_flush()\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\tqdm\\utils.py:196\u001b[0m, in \u001b[0;36mDisableOnWriteError.disable_on_exception.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    198\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39merrno \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m5\u001b[39m:\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\ipykernel\\iostream.py:662\u001b[0m, in \u001b[0;36mOutStream.write\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[0;32m    661\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 662\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schedule_flush()\n\u001b[0;32m    664\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(string)\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\ipykernel\\iostream.py:559\u001b[0m, in \u001b[0;36mOutStream._schedule_flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_schedule_in_thread\u001b[39m():\n\u001b[0;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_io_loop\u001b[38;5;241m.\u001b[39mcall_later(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_interval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[1;32m--> 559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(_schedule_in_thread)\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\ipykernel\\iostream.py:266\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[1;34m(self, f)\u001b[0m\n\u001b[0;32m    264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_pipe\u001b[38;5;241m.\u001b[39msend(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    268\u001b[0m     f()\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\zmq\\sugar\\socket.py:696\u001b[0m, in \u001b[0;36mSocket.send\u001b[1;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[0;32m    689\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[0;32m    690\u001b[0m             data,\n\u001b[0;32m    691\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[0;32m    692\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    693\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[0;32m    694\u001b[0m         )\n\u001b[0;32m    695\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msend(data, flags\u001b[38;5;241m=\u001b[39mflags, copy\u001b[38;5;241m=\u001b[39mcopy, track\u001b[38;5;241m=\u001b[39mtrack)\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:742\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:789\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket.Socket.send\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mzmq\\\\backend\\\\cython\\\\socket.pyx:250\u001b[0m, in \u001b[0;36mzmq.backend.cython.socket._send_copy\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\anaconda\\Lib\\site-packages\\zmq\\backend\\cython\\checkrc.pxd:13\u001b[0m, in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import snntorch as snn\n",
    "from snntorch import surrogate\n",
    "from snntorch import functional as SF\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "class SCNN(nn.Module):\n",
    "    def __init__(self, num_inputs=1, num_hidden=16, num_outputs=10, beta=0.95):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Input convolutional layer\n",
    "        self.conv1 = nn.Conv2d(num_inputs, num_hidden, kernel_size=5, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(num_hidden, num_hidden, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        self.conv3 = nn.Conv2d(num_hidden, 8, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2)\n",
    "        \n",
    "        # Calculate the size of flattened features\n",
    "        self.to_linear = None\n",
    "        self._get_conv_output_size((1, 28, 28))\n",
    "        \n",
    "        # Flattening and output layer\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(self.to_linear, num_outputs)\n",
    "        \n",
    "        # Spiking neuron dynamics\n",
    "        self.lif1 = snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid())\n",
    "        self.lif2 = snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid())\n",
    "        self.lif3 = snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid())\n",
    "        self.lif_out = snn.Leaky(beta=beta, spike_grad=surrogate.fast_sigmoid())\n",
    "        \n",
    "        # Initialize states\n",
    "        self.mem1 = self.mem2 = self.mem3 = self.mem_out = None\n",
    "    \n",
    "    def _get_conv_output_size(self, shape):\n",
    "        \"\"\"\n",
    "        Calculate the output size of the convolutional layers\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Create a dummy input tensor\n",
    "            batch_size = 1\n",
    "            input = torch.zeros(batch_size, *shape)\n",
    "            \n",
    "            # Pass it through the convolutional layers\n",
    "            x = self.conv1(input)\n",
    "            x = self.conv2(x)\n",
    "            x = self.pool1(x)\n",
    "            x = self.conv3(x)\n",
    "            x = self.pool2(x)\n",
    "            \n",
    "            # Get the flattened size\n",
    "            self.to_linear = x.numel()\n",
    "            print(f\"Flattened size for fc1: {self.to_linear}\")\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize or reset membrane potential at first timestep\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Calculate the output shape of each layer\n",
    "        conv1_shape = self._conv_output_shape(x.shape, kernel_size=5, stride=2, padding=1)\n",
    "        conv2_shape = self._conv_output_shape(conv1_shape, kernel_size=3, stride=1, padding=1)\n",
    "        pool1_shape = self._pool_output_shape(conv2_shape, kernel_size=2, stride=2, padding=0)\n",
    "        conv3_shape = self._conv_output_shape(pool1_shape, kernel_size=3, stride=1, padding=1)\n",
    "        pool2_shape = self._pool_output_shape(conv3_shape, kernel_size=2, stride=2, padding=0)\n",
    "        \n",
    "        if self.mem1 is None:\n",
    "            self.mem1 = torch.zeros(batch_size, 16, conv1_shape[0], conv1_shape[1], device=x.device)\n",
    "            self.mem2 = torch.zeros(batch_size, 16, conv2_shape[0], conv2_shape[1], device=x.device)\n",
    "            self.mem3 = torch.zeros(batch_size, 8, pool2_shape[0], pool2_shape[1], device=x.device)\n",
    "            self.mem_out = torch.zeros(batch_size, 10, device=x.device)\n",
    "        \n",
    "        # First convolutional layer with LIF neuron\n",
    "        cur1 = self.conv1(x)\n",
    "        spk1, self.mem1 = self.lif1(cur1, self.mem1)\n",
    "        \n",
    "        # Second convolutional layer with LIF neuron\n",
    "        cur2 = self.conv2(spk1)\n",
    "        spk2, self.mem2 = self.lif2(cur2, self.mem2)\n",
    "        spk2 = self.pool1(spk2)\n",
    "        \n",
    "        # Third convolutional layer with LIF neuron\n",
    "        cur3 = self.conv3(spk2)\n",
    "        spk3, self.mem3 = self.lif3(cur3, self.mem3)\n",
    "        spk3 = self.pool2(spk3)\n",
    "        \n",
    "        # Flatten and output layer\n",
    "        flat = self.flatten(spk3)\n",
    "        cur_out = self.fc1(flat)\n",
    "        spk_out, self.mem_out = self.lif_out(cur_out, self.mem_out)\n",
    "        \n",
    "        return spk_out\n",
    "    \n",
    "    def _conv_output_shape(self, input_shape, kernel_size, stride, padding):\n",
    "        \"\"\"\n",
    "        Calculate the output shape of a convolutional layer\n",
    "        \"\"\"\n",
    "        if isinstance(input_shape, torch.Size):\n",
    "            height, width = input_shape[-2], input_shape[-1]\n",
    "        else:\n",
    "            height, width = input_shape\n",
    "        \n",
    "        # Convert to tuples if not already\n",
    "        if isinstance(kernel_size, int):\n",
    "            kernel_size = (kernel_size, kernel_size)\n",
    "        if isinstance(stride, int):\n",
    "            stride = (stride, stride)\n",
    "        if isinstance(padding, int):\n",
    "            padding = (padding, padding)\n",
    "        \n",
    "        h_out = (height + 2 * padding[0] - kernel_size[0]) // stride[0] + 1\n",
    "        w_out = (width + 2 * padding[1] - kernel_size[1]) // stride[1] + 1\n",
    "        \n",
    "        return (h_out, w_out)\n",
    "    \n",
    "    def _pool_output_shape(self, input_shape, kernel_size, stride, padding):\n",
    "        \"\"\"\n",
    "        Calculate the output shape of a pooling layer\n",
    "        \"\"\"\n",
    "        return self._conv_output_shape(input_shape, kernel_size, stride, padding)\n",
    "    \n",
    "    def reset_states(self):\n",
    "        self.mem1 = self.mem2 = self.mem3 = self.mem_out = None\n",
    "\n",
    "# Load N-MNIST dataset\n",
    "def load_nmnist():\n",
    "    # Placeholder - in a real implementation, you would load the actual N-MNIST dataset\n",
    "    # This is just a mock implementation using MNIST for demonstration\n",
    "    transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = torchvision.datasets.MNIST(\n",
    "        root=\"./data\", train=True, download=True, transform=transform\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.MNIST(\n",
    "        root=\"./data\", train=False, download=True, transform=transform\n",
    "    )\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset, batch_size=128, shuffle=True, num_workers=0\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset, batch_size=128, shuffle=False, num_workers=0\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, optimizer, num_steps=100):\n",
    "    model.train()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    loss_fn = SF.ce_rate_loss()\n",
    "    \n",
    "    for epoch in range(5):  # 5 epochs\n",
    "        running_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for data, targets in tqdm(train_loader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Initialize spike recording\n",
    "            spike_record = []\n",
    "            \n",
    "            # Forward pass through time\n",
    "            for step in range(num_steps):\n",
    "                spk_out = model(data)\n",
    "                spike_record.append(spk_out)\n",
    "            \n",
    "            # Calculate loss\n",
    "            spike_record = torch.stack(spike_record, dim=0)\n",
    "            loss = loss_fn(spike_record, targets)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            pred = torch.sum(spike_record, dim=0).argmax(dim=1)\n",
    "            correct += (pred == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Reset model states for next iteration\n",
    "            model.reset_states()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}, Accuracy: {correct/total}\")\n",
    "\n",
    "# Test function\n",
    "def test(model, test_loader, num_steps=100):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, targets in tqdm(test_loader):\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Initialize spike recording\n",
    "            spike_record = []\n",
    "            \n",
    "            # Forward pass through time\n",
    "            for step in range(num_steps):\n",
    "                spk_out = model(data)\n",
    "                spike_record.append(spk_out)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            spike_record = torch.stack(spike_record, dim=0)\n",
    "            pred = torch.sum(spike_record, dim=0).argmax(dim=1)\n",
    "            correct += (pred == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            # Reset model states for next iteration\n",
    "            model.reset_states()\n",
    "    \n",
    "    print(f\"Test Accuracy: {correct/total}\")\n",
    "    return correct/total\n",
    "\n",
    "# Main training routine\n",
    "def main():\n",
    "    device = torch.device(\"cuda\")\n",
    "    \n",
    "    # Create model and optimizer\n",
    "    model = SCNN().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    # Load data\n",
    "    train_loader, test_loader = load_nmnist()\n",
    "    \n",
    "    # Train the model\n",
    "    train(model, train_loader, optimizer)\n",
    "    \n",
    "    # Test the model\n",
    "    accuracy = test(model, test_loader)\n",
    "    \n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"scnn_model.pth\")\n",
    "    return model\n",
    "\n",
    "# Run the training\n",
    "model = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Исправить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Добавляем директорию, содержащую модуль 'nir', в sys.path\n",
    "sys.path.append(\"F:/vs_file/NIR\")\n",
    "\n",
    "import nir\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened size for fc1: 72\n",
      "Создаем узлы...\n",
      "Ошибка при конвертации модели: Conv2d.__init__() got an unexpected keyword argument 'weights'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_16488\\3428602302.py\", line 22, in convert_to_nir\n",
      "    node_dict[conv1_node_name] = nir.Conv2d(\n",
      "                                 ^^^^^^^^^^^\n",
      "TypeError: Conv2d.__init__() got an unexpected keyword argument 'weights'\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import nir  # Ensure this module is correctly imported\n",
    "from nir.serialization import write  # Import the write function\n",
    "\n",
    "def convert_to_nir(model):\n",
    "    # Словарь для хранения узлов, ключи - имена узлов\n",
    "    node_dict = {}\n",
    "    # Список для хранения ребер\n",
    "    edges = []\n",
    "    \n",
    "    # Создаем узлы\n",
    "    \n",
    "    # Входной узел\n",
    "    input_node_name = \"input\"\n",
    "    node_dict[input_node_name] = nir.Input(input_type=np.array([1, 28, 28]))\n",
    "    \n",
    "    # Первый сверточный слой\n",
    "    conv1_node_name = \"conv1\"\n",
    "    node_dict[conv1_node_name] = nir.Conv2d(\n",
    "        input_shape=(28, 28),\n",
    "        weight=model.conv1.weight.data.cpu().numpy(),\n",
    "        stride=(2, 2),\n",
    "        padding=(1, 1),\n",
    "        dilation=(1, 1),\n",
    "        groups=1,\n",
    "        bias=model.conv1.bias.data.cpu().numpy() if model.conv1.bias is not None else None\n",
    "    )\n",
    "    \n",
    "    # Первый LIF слой\n",
    "    lif1_node_name = \"lif1\"\n",
    "    node_dict[lif1_node_name] = nir.LIF(\n",
    "        tau=np.array([1/(1-model.lif1.beta)]),\n",
    "        v_leak=np.array([0.0]),\n",
    "        r=np.array([1.0]),\n",
    "        v_threshold=np.array([1.0])\n",
    "    )\n",
    "    \n",
    "    # Второй сверточный слой\n",
    "    conv2_node_name = \"conv2\"\n",
    "    node_dict[conv2_node_name] = nir.Conv2d(\n",
    "        input_shape=(14, 14),  # Adjust input shape based on previous layer's output\n",
    "        weight=model.conv2.weight.data.cpu().numpy(),\n",
    "        stride=(1, 1),\n",
    "        padding=(1, 1),\n",
    "        dilation=(1, 1),\n",
    "        groups=1,\n",
    "        bias=model.conv2.bias.data.cpu().numpy() if model.conv2.bias is not None else None\n",
    "    )\n",
    "    \n",
    "    # Второй LIF слой\n",
    "    lif2_node_name = \"lif2\"\n",
    "    node_dict[lif2_node_name] = nir.LIF(\n",
    "        tau=np.array([1/(1-model.lif2.beta)]),\n",
    "        v_leak=np.array([0.0]),\n",
    "        r=np.array([1.0]),\n",
    "        v_threshold=np.array([1.0])\n",
    "    )\n",
    "    \n",
    "    # Первый пулинговый слой\n",
    "    pool1_node_name = \"pool1\"\n",
    "    node_dict[pool1_node_name] = nir.SumPool2d(\n",
    "        kernel_size=(2, 2),\n",
    "        stride=(2, 2),\n",
    "        padding=(0, 0)\n",
    "    )\n",
    "    \n",
    "    # Третий сверточный слой\n",
    "    conv3_node_name = \"conv3\"\n",
    "    node_dict[conv3_node_name] = nir.Conv2d(\n",
    "        input_shape=(7, 7),  # Adjust input shape based on previous layer's output\n",
    "        weight=model.conv3.weight.data.cpu().numpy(),\n",
    "        stride=(1, 1),\n",
    "        padding=(1, 1),\n",
    "        dilation=(1, 1),\n",
    "        groups=1,\n",
    "        bias=model.conv3.bias.data.cpu().numpy() if model.conv3.bias is not None else None\n",
    "    )\n",
    "    \n",
    "    # Третий LIF слой\n",
    "    lif3_node_name = \"lif3\"\n",
    "    node_dict[lif3_node_name] = nir.LIF(\n",
    "        tau=np.array([1/(1-model.lif3.beta)]),\n",
    "        v_leak=np.array([0.0]),\n",
    "        r=np.array([1.0]),\n",
    "        v_threshold=np.array([1.0])\n",
    "    )\n",
    "    \n",
    "    # Второй пулинговый слой\n",
    "    pool2_node_name = \"pool2\"\n",
    "    node_dict[pool2_node_name] = nir.SumPool2d(\n",
    "        kernel_size=(2, 2),\n",
    "        stride=(2, 2),\n",
    "        padding=(0, 0)\n",
    "    )\n",
    "    \n",
    "    # Слой Flatten\n",
    "    flatten_node_name = \"flatten\"\n",
    "    node_dict[flatten_node_name] = nir.Flatten(\n",
    "        start_dim=1,\n",
    "        end_dim=3\n",
    "    )\n",
    "    \n",
    "    # Полносвязный слой\n",
    "    fc1_node_name = \"fc1\"\n",
    "    node_dict[fc1_node_name] = nir.Affine(\n",
    "        weight=model.fc1.weight.data.cpu().numpy(),\n",
    "        bias=model.fc1.bias.data.cpu().numpy() if model.fc1.bias is not None else None\n",
    "    )\n",
    "    \n",
    "    # Выходной LIF слой\n",
    "    lif_out_node_name = \"lif_out\"\n",
    "    node_dict[lif_out_node_name] = nir.LIF(\n",
    "        tau=np.array([1/(1-model.lif_out.beta)]),\n",
    "        v_leak=np.array([0.0]),\n",
    "        r=np.array([1.0]),\n",
    "        v_threshold=np.array([1.0])\n",
    "    )\n",
    "    \n",
    "    # Выходной узел\n",
    "    output_node_name = \"output\"\n",
    "    node_dict[output_node_name] = nir.Output(output_type=np.array([10]))\n",
    "    \n",
    "    # Добавляем ребра\n",
    "    edges = [\n",
    "        (input_node_name, conv1_node_name),\n",
    "        (conv1_node_name, lif1_node_name),\n",
    "        (lif1_node_name, conv2_node_name),\n",
    "        (conv2_node_name, lif2_node_name),\n",
    "        (lif2_node_name, pool1_node_name),\n",
    "        (pool1_node_name, conv3_node_name),\n",
    "        (conv3_node_name, lif3_node_name),\n",
    "        (lif3_node_name, pool2_node_name),\n",
    "        (pool2_node_name, flatten_node_name),\n",
    "        (flatten_node_name, fc1_node_name),\n",
    "        (fc1_node_name, lif_out_node_name),\n",
    "        (lif_out_node_name, output_node_name)\n",
    "    ]\n",
    "    \n",
    "    # Создаем граф NIR\n",
    "    nir_graph = nir.NIRGraph(\n",
    "        nodes=node_dict,\n",
    "        edges=edges\n",
    "    )\n",
    "    \n",
    "    # Debug print to inspect the dictionary\n",
    "    nir_graph_dict = nir_graph.to_dict()\n",
    "    for key, value in nir_graph_dict.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Сохраняем граф\n",
    "    write(\"scnn_model.nir\", nir_graph)\n",
    "    \n",
    "    return nir_graph\n",
    "\n",
    "model = SCNN()\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(\"scnn_model.pth\"))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "# Convert the trained model to NIR\n",
    "nir_graph = convert_to_nir(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
